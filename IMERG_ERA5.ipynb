{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe938765",
   "metadata": {},
   "source": [
    "\n",
    "# üß™ Demo 5: Ground Truth Challenge ‚Äî Validate Your Forecast with Your Data (Demo 4‚Äìstyle)\n",
    "\n",
    "This notebook mirrors **Demo 4** (widgets, regions, WeatherBench‚ÄëX), and adds the ability to plug in **your own ground truth**:\n",
    "- **Default**: ERA5 from Google Cloud Storage (no downloads)\n",
    "- **Optional**: Point **CSV** (`time,lat,lon,value`) with time tolerance\n",
    "- **Optional**: **Gridded** NetCDF/Zarr file (local path or `gs://‚Ä¶`)\n",
    "\n",
    "You can compare **Global vs Local** regions and compute **RMSE** (and **ACC** if GT supports climatology).\n",
    "\n",
    "> Tip: start with ERA5 as truth; once it runs, try your CSV/NetCDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdf58205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & config ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import weatherbenchX\n",
    "from weatherbenchX.metrics import deterministic\n",
    "from weatherbenchX.metrics import base as metrics_base\n",
    "\n",
    "plt.rcParams.update({\"figure.dpi\": 120})\n",
    "\n",
    "# --- YOUR DEFAULT PATHS (edit if needed) ---\n",
    "FORECAST_DEFAULT = \"gs://aim4scale_training_25/ground_truth/IMERG_0p25_2000_2025.zarr\"   # or IMD_rainfall_0p25.zarr\n",
    "ERA5_DAILY_DEFAULT = \"gs://aim4scale_training_25/ground_truth/era5_24hr.zarr\"            # your ERA5-daily Zarr\n",
    "\n",
    "FORECAST_VAR = \"total_precipitation_24hr\"   # both IMERG/IMD expose this name in your bucket\n",
    "\n",
    "# --- Regions you asked for ---\n",
    "REGIONS = {\n",
    "    \"Global\":     {\"latitude\": slice( 90, -90), \"longitude\": slice(  0, 360)},\n",
    "    \"Ethiopia\":   {\"latitude\": slice(14.9,  3.4), \"longitude\": slice(33.0, 48.0)},\n",
    "    \"Nigeria\":    {\"latitude\": slice(14.7,  4.0), \"longitude\": slice( 2.7, 14.7)},\n",
    "    \"Kenya\":      {\"latitude\": slice( 5.0, -4.7), \"longitude\": slice(33.9, 41.9)},\n",
    "    \"Bangladesh\": {\"latitude\": slice(26.7, 20.7), \"longitude\": slice(88.0, 92.7)},\n",
    "    \"Chile\":      {\"latitude\": slice(-17.5,-56.0), \"longitude\": slice(284.0, 294.0)},\n",
    "}\n",
    "\n",
    "# ---------- small utilities ----------\n",
    "def _open_any(path: str):\n",
    "    return xr.open_zarr(path) if path.endswith(\".zarr\") else xr.open_dataset(path)\n",
    "\n",
    "def _to_0360(obj):\n",
    "    if \"longitude\" in obj.coords:\n",
    "        lon = obj[\"longitude\"]\n",
    "        try:\n",
    "            if float(lon.min()) < 0:\n",
    "                obj = obj.assign_coords(longitude=(lon % 360))\n",
    "        except Exception:\n",
    "            pass\n",
    "        obj = obj.sortby(\"longitude\")\n",
    "    return obj\n",
    "\n",
    "def _ensure_lat_ascending(obj):\n",
    "    if \"latitude\" in obj.coords:\n",
    "        lat = obj[\"latitude\"].values\n",
    "        if len(lat) > 1 and lat[0] > lat[-1]:\n",
    "            obj = obj.sortby(\"latitude\")\n",
    "    return obj\n",
    "\n",
    "def _region_to_0360(region):\n",
    "    a = float(region[\"longitude\"].start); b = float(region[\"longitude\"].stop)\n",
    "    return {\n",
    "        \"latitude\": slice(region[\"latitude\"].start, region[\"latitude\"].stop),\n",
    "        \"longitude\": slice(a % 360, b % 360),\n",
    "    }\n",
    "\n",
    "def _apply_region_safe(ds, region):\n",
    "    \"\"\"Slice safely even if lon systems differ. We convert DATA to 0..360, then slice with 0..360 bounds.\"\"\"\n",
    "    ds1 = _ensure_lat_ascending(_to_0360(ds))\n",
    "    r0360 = _region_to_0360(region)\n",
    "    lo = min(r0360[\"longitude\"].start, r0360[\"longitude\"].stop)\n",
    "    hi = max(r0360[\"longitude\"].start, r0360[\"longitude\"].stop)\n",
    "    lat_lo = min(region[\"latitude\"].start, region[\"latitude\"].stop)\n",
    "    lat_hi = max(region[\"latitude\"].start, region[\"latitude\"].stop)\n",
    "    return ds1.sel(latitude=slice(lat_lo, lat_hi), longitude=slice(lo, hi))\n",
    "\n",
    "def _normalize_precip_units(da):\n",
    "    \"\"\"unify precip to mm/day.\"\"\"\n",
    "    units = (da.attrs.get(\"units\") or da.attrs.get(\"unit\") or \"\").lower()\n",
    "    out = da\n",
    "    if units in [\"m\", \"meter\", \"metre\", \"m of water equivalent\"]:\n",
    "        out = out * 1000.0\n",
    "        out.attrs[\"units\"] = \"mm\"\n",
    "    elif units in [\"kg m-2\", \"kg/m^2\", \"kg m**-2\"]:\n",
    "        # 1 kg m-2 ‚âà 1 mm\n",
    "        out.attrs[\"units\"] = \"mm\"\n",
    "    # if units empty, assume mm (your datasets are mm already)\n",
    "    return out\n",
    "\n",
    "def _coerce_valid_time(da):\n",
    "    \"\"\"WeatherBenchX accepts either valid_time or (init_time, lead_time). We'll use valid_time.\"\"\"\n",
    "    if \"valid_time\" in da.dims:\n",
    "        return da\n",
    "    if \"time\" in da.dims:\n",
    "        return da.rename({\"time\": \"valid_time\"})\n",
    "    # try to decode CF times if needed\n",
    "    try:\n",
    "        dec = xr.decode_cf(da.to_dataset(name=\"_tmp\")).to_array(\"_tmp\")\n",
    "        if \"time\" in dec.dims:\n",
    "            return dec.rename({\"time\": \"valid_time\"})\n",
    "    except Exception:\n",
    "        pass\n",
    "    raise ValueError(\"No recognizable time/valid_time dimension in array.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed113ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883b84eddb124794827e4b13709c6ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='gs://aim4scale_training_25/ground_truth/IMERG_0p25_2000_2025.zarr', description='Forecast (IMERG/I‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33580a7b8a4481a96677e21e5eb71e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='gs://aim4scale_training_25/ground_truth/era5_24hr.zarr', description='ERA5 daily:', layout=Layout(‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddac81ae8bc46ada187334a4bf22c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Load', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8c39502135438ab2ddbf97c9d04889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch applied. Click 'Load' again, then use 'Run verification'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61249d4b0c36426e9d345417ef170842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Region:', options=('Global', 'Ethiopia', 'Nigeria', 'Kenya', 'Bangladesh', 'Chile'), val‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee42bef34f154e5c997916f5f3f5b314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Metric:', options=('RMSE', 'MAE', 'ACC'), value='RMSE')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09cf9e427d5546e5b0bfccaf2f4353f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Variable:', options=('total_precipitation_24hr',), value='total_precipitation_24hr')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228312be2e954d9f9d87e8d36896f317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='2023-01-01', description='Start (YYYY-MM-DD):')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5581ff4b90284660a4327cf30f9f8c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='2023-01-31', description='End   (YYYY-MM-DD):')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859cdcba28be44629c29701ba8b0af51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Run verification', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bab405312a492b9d3d0ffbe7e6d730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚û°Ô∏è Load the datasets above first."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------- loader widgets --------\n",
    "fc_path = widgets.Text(\n",
    "    value=FORECAST_DEFAULT,\n",
    "    description=\"Forecast (IMERG/IMD):\",\n",
    "    layout=widgets.Layout(width=\"95%\")\n",
    ")\n",
    "era5_path = widgets.Text(\n",
    "    value=ERA5_DAILY_DEFAULT,\n",
    "    description=\"ERA5 daily:\",\n",
    "    layout=widgets.Layout(width=\"95%\")\n",
    ")\n",
    "load_btn = widgets.Button(description=\"Load\", button_style=\"primary\")\n",
    "load_out = widgets.Output()\n",
    "display(fc_path, era5_path, load_btn, load_out)\n",
    "\n",
    "forecast_ds = None\n",
    "era5_daily_ds = None\n",
    "\n",
    "def _summ(ds):\n",
    "    vs = \", \".join(list(ds.data_vars)[:6])\n",
    "    return f\"vars: {vs} | sizes: {dict(ds.sizes)}\"\n",
    "\n",
    "# --- Patch: re-launch verification UI after datasets load ---\n",
    "\n",
    "def _on_load(_):\n",
    "    global forecast_ds, era5_daily_ds\n",
    "    with load_out:\n",
    "        load_out.clear_output()\n",
    "        try:\n",
    "            forecast_ds    = _open_any(fc_path.value.strip())\n",
    "            era5_daily_ds  = _open_any(era5_path.value.strip())\n",
    "            display(Markdown(\"‚úÖ **Loaded datasets**\"))\n",
    "            display(Markdown(f\"- Forecast ‚Üí `{_summ(forecast_ds)}`\"))\n",
    "            display(Markdown(f\"- ERA5 daily ‚Üí `{_summ(era5_daily_ds)}`\"))\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"‚ùå Load error: `{e}`\"))\n",
    "            return\n",
    "\n",
    "    # IMPORTANT: (re)launch the verification UI so the callbacks\n",
    "    # capture the newly loaded datasets instead of the old Nones.\n",
    "    verify_interactive()\n",
    "\n",
    "# re-bind the click handler (overwrites the old one)\n",
    "load_btn.on_click(_on_load)\n",
    "\n",
    "print(\"Patch applied. Click 'Load' again, then use 'Run verification'.\")\n",
    "\n",
    "# -------- verification UI (precip only) --------\n",
    "region_dd = widgets.Dropdown(options=list(REGIONS.keys()), description=\"Region:\", value=\"Global\")\n",
    "metric_dd = widgets.Dropdown(options=[\"RMSE\", \"MAE\", \"ACC\"], description=\"Metric:\", value=\"RMSE\")\n",
    "var_dd    = widgets.Dropdown(options=[FORECAST_VAR], description=\"Variable:\", value=FORECAST_VAR)\n",
    "\n",
    "# simple time pickers\n",
    "t0 = widgets.Text(value=\"2023-01-01\", description=\"Start (YYYY-MM-DD):\")\n",
    "t1 = widgets.Text(value=\"2023-01-31\", description=\"End   (YYYY-MM-DD):\")\n",
    "\n",
    "run_btn = widgets.Button(description=\"Run verification\", button_style=\"success\")\n",
    "out = widgets.Output()\n",
    "display(region_dd, metric_dd, var_dd, t0, t1, run_btn, out)\n",
    "\n",
    "def _nearest_regrid_truth_to_forecast(truth_da, forecast_da):\n",
    "    \"\"\"Nearest-neighbor ERA5‚Üíforecast grid (both 0.25¬∞, but we still guard).\"\"\"\n",
    "    t = _ensure_lat_ascending(_to_0360(truth_da))\n",
    "    f = _ensure_lat_ascending(_to_0360(forecast_da))\n",
    "    return t.interp(latitude=f[\"latitude\"], longitude=f[\"longitude\"], method=\"nearest\")\n",
    "\n",
    "def _acc_from_stats(stats_ds, var):\n",
    "    \"\"\"Compute ACC from WBX outputs (already on time/valid_time).\"\"\"\n",
    "    # WBX returns components for ACC when we ask for ACC metric\n",
    "    pa  = stats_ds['SquaredPredictionAnomaly'][var].squeeze()\n",
    "    ta  = stats_ds['SquaredTargetAnomaly'][var].squeeze()\n",
    "    cov = stats_ds['AnomalyCovariance'][var].squeeze()\n",
    "    return (cov / np.sqrt(pa * ta))\n",
    "\n",
    "def verify_interactive():\n",
    "    if forecast_ds is None or era5_daily_ds is None:\n",
    "        display(Markdown(\"\")); \n",
    "        return\n",
    "\n",
    "    def on_run(_):\n",
    "        with out:\n",
    "            out.clear_output()\n",
    "\n",
    "            region = REGIONS[region_dd.value]\n",
    "            var = var_dd.value\n",
    "            metric = metric_dd.value\n",
    "            start = np.datetime64(pd.to_datetime(t0.value).date())\n",
    "            end   = np.datetime64(pd.to_datetime(t1.value).date())\n",
    "\n",
    "            print(f\"Selected region: {region_dd.value}\")\n",
    "            print(f\"Selected metric: {metric}\")\n",
    "            print(f\"Selected variable: {var}\")\n",
    "            print(f\"Time window: {str(start)} ‚Üí {str(end)}\")\n",
    "\n",
    "            # --- slice to region and time; normalize lon/lat conventions ---\n",
    "            # forecast: IMERG / IMD (already daily)\n",
    "            if var not in forecast_ds:\n",
    "                print(f\"‚ùå '{var}' not found in forecast dataset vars: {list(forecast_ds.data_vars)[:6]} ‚Ä¶\")\n",
    "                return\n",
    "            f = _apply_region_safe(forecast_ds[[var]], region)[var].sel(time=slice(start, end))\n",
    "            if f.sizes.get(\"time\", 0) == 0:\n",
    "                print(\"‚ö†Ô∏è Region/time window has no points on the forecast grid.\"); return\n",
    "            f = _normalize_precip_units(f)\n",
    "            f = _coerce_valid_time(f)\n",
    "\n",
    "            # ERA5 daily truth (same variable name)\n",
    "            if var not in era5_daily_ds:\n",
    "                print(f\"‚ùå '{var}' not found in ERA5 dataset vars: {list(era5_daily_ds.data_vars)[:6]} ‚Ä¶\")\n",
    "                return\n",
    "            print(\"Preparing ERA5 daily truth...\")\n",
    "            t = _apply_region_safe(era5_daily_ds[[var]], region)[var].sel(time=slice(start, end))\n",
    "            if t.sizes.get(\"time\", 0) == 0:\n",
    "                print(\"‚ö†Ô∏è Region/time window has no points in ERA5.\"); return\n",
    "            t = _normalize_precip_units(t)\n",
    "            t = _coerce_valid_time(t)\n",
    "\n",
    "            # regrid ERA5 ‚Üí forecast grid (guard if lat/lon differ)\n",
    "            t_on_f = _nearest_regrid_truth_to_forecast(t, f)\n",
    "\n",
    "            # intersect time just in case\n",
    "            tmin = max(np.datetime64(f[\"valid_time\"].min().values), np.datetime64(t_on_f[\"valid_time\"].min().values))\n",
    "            tmax = min(np.datetime64(f[\"valid_time\"].max().values), np.datetime64(t_on_f[\"valid_time\"].max().values))\n",
    "            f_use = f.sel(valid_time=slice(tmin, tmax))\n",
    "            t_use = t_on_f.sel(valid_time=slice(tmin, tmax))\n",
    "\n",
    "            if f_use.sizes.get(\"valid_time\", 0) == 0:\n",
    "                print(\"‚ö†Ô∏è No overlapping times between forecast and ERA5.\"); return\n",
    "\n",
    "            # WeatherBenchX expects datasets; give it valid_time dimension\n",
    "            preds = xr.Dataset({var: f_use})\n",
    "            targs = xr.Dataset({var: t_use})\n",
    "\n",
    "            # Metrics (ACC needs a DOY climatology on the same grid)\n",
    "            if metric == \"ACC\":\n",
    "                print(\"Building daily day-of-year climatology for ACC‚Ä¶\")\n",
    "                clim_doy = (\n",
    "                    t_use.groupby(\"valid_time.dayofyear\")\n",
    "                         .mean(\"valid_time\", keep_attrs=True)\n",
    "                         .rename(\"clim\")\n",
    "                )\n",
    "                # WBX requires a dataset where variable name matches:\n",
    "                clim = xr.Dataset({var: clim_doy})\n",
    "                metrics = {\"acc\": deterministic.ACC(climatology=clim)}\n",
    "            elif metric == \"RMSE\":\n",
    "                metrics = {\"rmse\": deterministic.RMSE()}\n",
    "            else:  # MAE\n",
    "                metrics = {\"mae\": deterministic.MAE()}\n",
    "\n",
    "            print(\"Computing statistics‚Ä¶\")\n",
    "            stats = metrics_base.compute_unique_statistics_for_all_metrics(metrics, preds, targs)\n",
    "            print(\"Done ‚úÖ\")\n",
    "\n",
    "            # reduce to a time series (area-mean over region)\n",
    "            if metric == \"RMSE\":\n",
    "                se = stats[\"SquaredError\"][var]\n",
    "                series = np.sqrt(se).mean(dim=[d for d in [\"latitude\",\"longitude\"] if d in se.dims])\n",
    "                ylabel = \"RMSE (mm/day)\"\n",
    "            elif metric == \"MAE\":\n",
    "                ae = stats[\"AbsoluteError\"][var]\n",
    "                series = ae.mean(dim=[d for d in [\"latitude\",\"longitude\"] if d in ae.dims])\n",
    "                ylabel = \"MAE (mm/day)\"\n",
    "            else:\n",
    "                acc = _acc_from_stats(stats, var)\n",
    "                series = acc.mean(dim=[d for d in [\"latitude\",\"longitude\"] if d in acc.dims])\n",
    "                ylabel = \"ACC\"\n",
    "            \n",
    "            # report number of valid days\n",
    "            print(f\"{int(series.sizes.get('valid_time', 0))} valid days in window.\")\n",
    "\n",
    "            # plot\n",
    "            fig, ax = plt.subplots(figsize=(8,4))\n",
    "            series.rename({\"valid_time\": \"time\"}).plot(ax=ax)\n",
    "            ax.set_title(f\"{metric} ‚Äî {('tp_24h' if var=='total_precipitation_24hr' else var)} ‚Äî {region_dd.value}\")\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "\n",
    "    run_btn.on_click(on_run)\n",
    "\n",
    "# launch interactive\n",
    "verify_interactive()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_path = widgets.Text(\n",
    "    value=FORECAST_DEFAULT,\n",
    "    description=\"Forecast Zarr:\",\n",
    "    layout=widgets.Layout(width=\"95%\")\n",
    ")\n",
    "era5_t_path = widgets.Text(\n",
    "    value=ERA5_TEMP_DAILY_DEFAULT,\n",
    "    description=\"ERA5 temp:\",\n",
    "    layout=widgets.Layout(width=\"95%\")\n",
    ")\n",
    "era5_p_path = widgets.Text(\n",
    "    value=ERA5_PRECIP_DAILY_DEFAULT,\n",
    "    description=\"ERA5 precip:\",\n",
    "    layout=widgets.Layout(width=\"95%\")\n",
    ")\n",
    "load_btn = widgets.Button(description=\"Load datasets\", button_style=\"primary\")\n",
    "load_out = widgets.Output()\n",
    "\n",
    "display(fc_path, era5_t_path, era5_p_path, load_btn, load_out)\n",
    "\n",
    "forecast_ds = None\n",
    "era5_temp_ds = None\n",
    "era5_precip_ds = None\n",
    "\n",
    "def _open_any(path: str):\n",
    "    if path.endswith(\".zarr\"):\n",
    "        return xr.open_zarr(path)\n",
    "    return xr.open_dataset(path)\n",
    "\n",
    "def _summ(ds):\n",
    "    vs = \", \".join([v for v in ds.data_vars][:6])\n",
    "    return f\"vars: {vs} ... | dims: {dict(ds.dims)}\"\n",
    "\n",
    "def _load_all(_):\n",
    "    global forecast_ds, era5_temp_ds, era5_precip_ds\n",
    "    with load_out:\n",
    "        load_out.clear_output()\n",
    "        try:\n",
    "            forecast_ds   = _open_any(fc_path.value.strip())\n",
    "            era5_temp_ds  = _open_any(era5_t_path.value.strip())\n",
    "            era5_precip_ds= _open_any(era5_p_path.value.strip())\n",
    "            display(Markdown(\"‚úÖ Loaded:\\n\"\n",
    "                             f\"- Forecast ‚Üí `{_summ(forecast_ds)}`\\n\"\n",
    "                             f\"- ERA5 temp daily ‚Üí `{_summ(era5_temp_ds)}`\\n\"\n",
    "                             f\"- ERA5 precip daily ‚Üí `{_summ(era5_precip_ds)}`\"))\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"‚ùå Load error: `{e}`\"))\n",
    "\n",
    "load_btn.on_click(_load_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VERIFY INTERACTIVE (daily ERA5 <-> your daily forecast) ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "from weatherbenchX.metrics import deterministic\n",
    "from weatherbenchX.metrics import base as metrics_base\n",
    "\n",
    "# Regions (same as before)\n",
    "DOMAIN_DEFINITIONS = {\n",
    "    \"Global\": {\"latitude\": slice(90, -90), \"longitude\": slice(0, 360)},\n",
    "    \"Ethiopia\": {\"latitude\": slice(14.9, 3.4), \"longitude\": slice(33.0, 48.0)},\n",
    "    \"Nigeria\": {\"latitude\": slice(14.7, 4.0), \"longitude\": slice(2.7, 14.7)},\n",
    "    \"Kenya\": {\"latitude\": slice(5.0, -4.7), \"longitude\": slice(33.9, 41.9)},\n",
    "    \"Bangladesh\": {\"latitude\": slice(26.7, 20.7), \"longitude\": slice(88.0, 92.7)},\n",
    "    \"Chile\": {\"latitude\": slice(-17.5, -56.0), \"longitude\": slice(284.0, 294.0)},\n",
    "}\n",
    "\n",
    "DAILY_ALLOWED = [\"tmin\", \"tmax\", \"tavg\", \"total_precipitation_24hr\"]\n",
    "\n",
    "# ---- helpers: coords/units/time ----\n",
    "def _has_latlon(da): return \"latitude\" in da.coords and \"longitude\" in da.coords\n",
    "\n",
    "def _to0360(obj):\n",
    "    if \"longitude\" in obj.coords:\n",
    "        lon = obj[\"longitude\"]\n",
    "        if float(lon.min()) < 0:\n",
    "            obj = obj.assign_coords(longitude=(lon % 360)).sortby(\"longitude\")\n",
    "    return obj\n",
    "\n",
    "def _lat_ascending(obj):\n",
    "    if \"latitude\" in obj.coords:\n",
    "        lat = obj[\"latitude\"].values\n",
    "        if lat[0] > lat[-1]:\n",
    "            obj = obj.sortby(\"latitude\")\n",
    "    return obj\n",
    "\n",
    "def _norm_grid(obj):\n",
    "    return _lat_ascending(_to0360(obj))\n",
    "\n",
    "def _slice_region_safe(da_or_ds, region):\n",
    "    \"\"\"Slice even if dataset uses different lon convention; we convert to 0..360 and slice.\"\"\"\n",
    "    if not _has_latlon(da_or_ds):\n",
    "        return da_or_ds\n",
    "    ds = _norm_grid(da_or_ds)\n",
    "    # 0..360 region bounds\n",
    "    a = float(region[\"longitude\"].start); b = float(region[\"longitude\"].stop)\n",
    "    lo_lon, hi_lon = (a % 360, b % 360)\n",
    "    lo_lon, hi_lon = (min(lo_lon, hi_lon), max(lo_lon, hi_lon))\n",
    "    # latitude bounds (ascending)\n",
    "    la = float(region[\"latitude\"].start); lb = float(region[\"latitude\"].stop)\n",
    "    lo_lat, hi_lat = (min(la, lb), max(la, lb))\n",
    "    return ds.sel(latitude=slice(lo_lat, hi_lat), longitude=slice(lo_lon, hi_lon))\n",
    "\n",
    "def _coerce_time(da: xr.DataArray) -> xr.DataArray:\n",
    "    for k in [\"time\", \"date\", \"day\", \"valid_time\"]:\n",
    "        if k in da.dims:\n",
    "            return da.rename({k: \"time\"})\n",
    "    # last resort: try decode_cf\n",
    "    dec = xr.decode_cf(da.to_dataset(name=\"_tmp\"), use_cftime=False).to_array(\"_tmp\")\n",
    "    for k in [\"time\", \"date\", \"day\", \"valid_time\"]:\n",
    "        if k in dec.dims:\n",
    "            return dec.rename({k: \"time\"})\n",
    "    raise ValueError(\"Daily variable is missing a recognizable time dimension.\")\n",
    "\n",
    "def _normalize_units(da: xr.DataArray, name: str) -> xr.DataArray:\n",
    "    units = (da.attrs.get(\"units\") or da.attrs.get(\"unit\") or \"\").lower()\n",
    "    out = da\n",
    "    if name in {\"tmin\", \"tmax\", \"tavg\"}:\n",
    "        if units in [\"c\", \"degc\", \"celsius\", \"¬∞c\"]:\n",
    "            out = out + 273.15\n",
    "        out.attrs[\"units\"] = \"K\"\n",
    "    elif name == \"total_precipitation_24hr\":\n",
    "        # Expect mm. If meters, convert.\n",
    "        if units in [\"m\", \"meter\", \"metre\", \"m of water equivalent\"]:\n",
    "            out = out * 1000.0\n",
    "        out.attrs[\"units\"] = \"mm\"\n",
    "    return out\n",
    "\n",
    "def _match_lon_system(src: xr.DataArray, tgt: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"Return 'src' with longitudes converted to match 'tgt' convention.\"\"\"\n",
    "    if \"longitude\" not in src.coords or \"longitude\" not in tgt.coords:\n",
    "        return src\n",
    "    lon_s = src.longitude.values\n",
    "    lon_t = tgt.longitude.values\n",
    "    if np.nanmin(lon_s) < 0 and np.nanmin(lon_t) >= 0:\n",
    "        src = src.assign_coords(longitude=(src.longitude % 360))\n",
    "    elif np.nanmin(lon_s) >= 0 and np.nanmin(lon_t) < 0:\n",
    "        src = src.assign_coords(longitude=((src.longitude + 180) % 360) - 180)\n",
    "    return src.sortby(\"longitude\")\n",
    "\n",
    "def _align_truth_to_forecast_grid(truth_da: xr.DataArray, forecast_da: xr.DataArray) -> xr.DataArray:\n",
    "    t = _lat_ascending(_to0360(truth_da))\n",
    "    f = _lat_ascending(_to0360(forecast_da))\n",
    "    # ensure same lon system\n",
    "    t = _match_lon_system(t, f)\n",
    "    # nearest-neighbour regrid (fast, avoids xesmf dependency)\n",
    "    return t.interp(latitude=f.latitude, longitude=f.longitude, method=\"nearest\")\n",
    "\n",
    "def _forecast_extent_region(f_ds) -> dict:\n",
    "    \"\"\"Use the forecast native extent as a 'region'.\"\"\"\n",
    "    lat = np.array(f_ds.latitude.values)\n",
    "    lon = np.array((_to0360(f_ds).longitude.values))\n",
    "    return {\n",
    "        \"latitude\": slice(float(lat.min()), float(lat.max())),\n",
    "        \"longitude\": slice(float(lon.min()), float(lon.max()))\n",
    "    }\n",
    "\n",
    "# ---- UI + engine ----\n",
    "def verify_interactive(forecast_ds, era5_temp_ds, era5_precip_ds):\n",
    "    # only show vars that actually exist in forecast\n",
    "    options = [v for v in DAILY_ALLOWED if v in forecast_ds.data_vars]\n",
    "    if not options:\n",
    "        display(Markdown(\"‚ùå No daily variables found in your forecast (expected any of \"\n",
    "                         \"`tmin`, `tmax`, `tavg`, `total_precipitation_24hr`).\"))\n",
    "        return\n",
    "\n",
    "    region_dd  = widgets.Dropdown(options=list(DOMAIN_DEFINITIONS.keys()),\n",
    "                                  description=\"Region:\", value=\"Global\")\n",
    "    metric_dd  = widgets.Dropdown(options=[\"RMSE\",\"MAE\",\"ACC\"],\n",
    "                                  description=\"Metric:\", value=\"RMSE\")\n",
    "    var_dd     = widgets.Dropdown(options=options, description=\"Variable:\", value=options[0])\n",
    "    days_box   = widgets.IntSlider(description=\"Days (latest):\", value=30, min=7, max=365, step=1)\n",
    "    run_btn    = widgets.Button(description=\"Run verification\", button_style=\"success\")\n",
    "    out        = widgets.Output()\n",
    "\n",
    "    display(region_dd, metric_dd, var_dd, days_box, run_btn, out)\n",
    "\n",
    "    def on_run(_):\n",
    "        with out:\n",
    "            out.clear_output()\n",
    "            var    = var_dd.value\n",
    "            metric = metric_dd.value\n",
    "            region_name = region_dd.value\n",
    "            print(f\"Selected region: {region_name}\")\n",
    "            print(f\"Selected metric: {metric}\")\n",
    "            print(f\"Selected variable: {var}\")\n",
    "            print(\"Preparing ERA5 daily truth...\")\n",
    "\n",
    "            # Forecast slice (region + last N days)\n",
    "            f_da = forecast_ds[var]\n",
    "            if not _has_latlon(f_da):\n",
    "                display(Markdown(\"‚ùå Forecast variable has no `latitude/longitude` coordinates.\")); return\n",
    "            f_da = _coerce_time(f_da)\n",
    "\n",
    "            # Region logic\n",
    "            if region_name == \"Global\":\n",
    "                # Use forecast native extent to avoid huge loads\n",
    "                region = _forecast_extent_region(forecast_ds)\n",
    "            else:\n",
    "                region = DOMAIN_DEFINITIONS[region_name]\n",
    "\n",
    "            f_da = _slice_region_safe(f_da, region)\n",
    "            if f_da.sizes.get(\"latitude\",0) == 0 or f_da.sizes.get(\"longitude\",0) == 0:\n",
    "                display(Markdown(\"‚ö†Ô∏è Region has no points on the forecast grid.\")); return\n",
    "\n",
    "            # restrict to the latest N days present in the forecast\n",
    "            tmax_f = pd.to_datetime(f_da.time.max().item())\n",
    "            tmin_f = tmax_f - pd.Timedelta(days=int(days_box.value)-1)\n",
    "            f_da = f_da.sel(time=slice(tmin_f, tmax_f))\n",
    "\n",
    "            # Units\n",
    "            f_da = _normalize_units(f_da, var)\n",
    "\n",
    "            # ERA5 \"truth\" (already DAILY in your stores)\n",
    "            if var in (\"tmin\",\"tmax\",\"tavg\"):\n",
    "                t_da = era5_temp_ds[var]\n",
    "            else:\n",
    "                t_da = era5_precip_ds[\"total_precipitation_24hr\"]\n",
    "\n",
    "            t_da = _slice_region_safe(t_da, region).sel(time=slice(tmin_f, tmax_f))\n",
    "            if t_da.sizes.get(\"latitude\",0) == 0 or t_da.sizes.get(\"longitude\",0) == 0:\n",
    "                display(Markdown(\"‚ö†Ô∏è Region has no points in the ERA5 dataset.\")); return\n",
    "\n",
    "            # grid normalisation & regrid ERA5 -> forecast grid\n",
    "            f_da = _norm_grid(f_da)\n",
    "            t_da = _norm_grid(t_da)\n",
    "            t_da_on_f = _align_truth_to_forecast_grid(t_da, f_da)\n",
    "\n",
    "            # Intersect times safely\n",
    "            t0 = max(np.datetime64(f_da.time.min().values), np.datetime64(t_da_on_f.time.min().values))\n",
    "            t1 = min(np.datetime64(f_da.time.max().values), np.datetime64(t_da_on_f.time.max().values))\n",
    "            f_da = f_da.sel(time=slice(t0, t1))\n",
    "            t_da_on_f = t_da_on_f.sel(time=slice(t0, t1))\n",
    "            if f_da.sizes.get(\"time\",0) == 0:\n",
    "                display(Markdown(\"‚ö†Ô∏è No overlapping time between forecast and ERA5 in that window.\")); return\n",
    "\n",
    "            # Build predictions/targets for weatherbenchX\n",
    "            # (ACC expects 'valid_time' or init/lead; we use valid_time)\n",
    "            pred = xr.Dataset({var: f_da.rename({\"time\":\"valid_time\"})})\n",
    "            targ = xr.Dataset({var: t_da_on_f.rename({\"time\":\"valid_time\"})})\n",
    "\n",
    "            # Metrics\n",
    "            if metric == \"ACC\":\n",
    "                # Day-of-year climatology from ERA5 daily (same region, same grid), across ALL years\n",
    "                # Slice ERA5 full time for this region, then regrid to forecast grid and compute DOY mean.\n",
    "                if var in (\"tmin\",\"tmax\",\"tavg\"):\n",
    "                    t_full = _slice_region_safe(era5_temp_ds[var], region)\n",
    "                else:\n",
    "                    t_full = _slice_region_safe(era5_precip_ds[\"total_precipitation_24hr\"], region)\n",
    "                t_full = _norm_grid(t_full)\n",
    "                t_full_on_f = _align_truth_to_forecast_grid(t_full, f_da)\n",
    "                clim_doy = t_full_on_f.groupby(\"time.dayofyear\").mean(\"time\")  # (dayofyear, lat, lon)\n",
    "                clim = xr.Dataset({var: clim_doy})\n",
    "                metrics = {\"acc\": deterministic.ACC(climatology=clim)}\n",
    "            elif metric == \"RMSE\":\n",
    "                metrics = {\"rmse\": deterministic.RMSE()}\n",
    "            else:  # MAE\n",
    "                metrics = {\"mae\": deterministic.MAE()}\n",
    "\n",
    "            # Compute\n",
    "            stats = metrics_base.compute_unique_statistics_for_all_metrics(metrics, pred, targ)\n",
    "\n",
    "            # Plot\n",
    "            if \"rmse\" in metrics:\n",
    "                se = stats[\"SquaredError\"][var]\n",
    "                series = np.sqrt(se).mean(dim=[d for d in [\"latitude\",\"longitude\"] if d in se.dims])\n",
    "                ylabel = \"RMSE\"\n",
    "            elif \"mae\" in metrics:\n",
    "                ae = stats[\"AbsoluteError\"][var]\n",
    "                series = ae.mean(dim=[d for d in [\"latitude\",\"longitude\"] if d in ae.dims])\n",
    "                ylabel = \"MAE\"\n",
    "            else:  # ACC\n",
    "                pa  = stats[\"SquaredPredictionAnomaly\"][var]\n",
    "                ta  = stats[\"SquaredTargetAnomaly\"][var]\n",
    "                cov = stats[\"AnomalyCovariance\"][var]\n",
    "                series = (cov/np.sqrt(pa*ta)).mean(dim=[d for d in [\"latitude\",\"longitude\"] if d in cov.dims])\n",
    "                ylabel = \"ACC\"\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(8,4))\n",
    "            series.to_series().plot(ax=ax)\n",
    "            ax.set_title(f\"{metric} ‚Äî {var} ({region_name})  [{pd.to_datetime(str(t0)).date()} ‚Ä¶ {pd.to_datetime(str(t1)).date()}]\")\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "\n",
    "    run_btn.on_click(on_run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b50ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clim():\n",
    "    var_map = {\n",
    "        \"2t\": \"2m_temperature\",\n",
    "        \"z_500\": \"geopotential\",\n",
    "        \"tp\": \"total_precipitation_6hr\",\n",
    "    }\n",
    "    clim = xr.open_zarr(\n",
    "        \"gs://weatherbench2/datasets/era5-hourly-climatology/1990-2019_6h_1440x721.zarr\",\n",
    "        consolidated=True\n",
    "    )\n",
    "    clim_var_map = {v: k for k, v in var_map.items()}\n",
    "    clim = clim.rename_vars(clim_var_map)\n",
    "    clim = clim[list(var_map.keys())]\n",
    "    clim[\"z_500\"] = clim[\"z_500\"].sel(level=500).drop_vars(\"level\")\n",
    "    return clim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8650e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics_in_chunks(forecast_ds, era5_ds, metrics, fvars, chunk_size=48):\n",
    "    \"\"\"Chunked compute, but only for the selected variables (fvars).\"\"\"\n",
    "    n = forecast_ds.sizes[\"init_time\"]\n",
    "    parts = []\n",
    "    for i in range(0, n, chunk_size):\n",
    "        sl = slice(i, min(i + chunk_size, n))\n",
    "        # ‚¨áÔ∏è subset to the exact variables on BOTH sides\n",
    "        f_chunk = forecast_ds[fvars].isel(init_time=sl)\n",
    "        t_chunk = era5_ds[fvars].isel(time=sl)\n",
    "        stats = metrics_base.compute_unique_statistics_for_all_metrics(metrics, f_chunk, t_chunk)\n",
    "        parts.append(stats)\n",
    "    final = {}\n",
    "    for st in parts:\n",
    "        for k, v in st.items():\n",
    "            final[k] = xr.concat([final[k], v], dim=\"time\") if k in final else v\n",
    "    return final\n",
    "\n",
    "def run_verification(forecast_ds, era5_ds, metric_name, fvars):\n",
    "    \"\"\"Compute metrics on the FULL grid (no region slicing here).\"\"\"\n",
    "    if metric_name == \"RMSE\":\n",
    "        mets = {\"rmse\": deterministic.RMSE()}\n",
    "    else:\n",
    "        clim = load_climatology_for_acc(fvars)\n",
    "        mets = {\"ACC\": deterministic.ACC(climatology=clim)}\n",
    "    return compute_statistics_in_chunks(forecast_ds, era5_ds, mets, fvars=fvars, chunk_size=48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac3e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 7: WB-X RMSE/ACC with safe reduction & plotting ---\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def _lat_slice_for(da: xr.DataArray, box):\n",
    "    lat = da[\"latitude\"]\n",
    "    a, b = box[\"latitude\"].start, box[\"latitude\"].stop\n",
    "    return slice(min(a,b), max(a,b)) if lat[0] < lat[-1] else slice(max(a,b), min(a,b))\n",
    "\n",
    "def get_lead_coord(da: xr.DataArray):\n",
    "    if \"lead_time\" in da.dims:\n",
    "        return \"lead_time\"\n",
    "    if \"step\" in da.dims:\n",
    "        return \"step\"\n",
    "    raise ValueError(f\"No lead dimension in {da.dims}\")\n",
    "\n",
    "def to_1d_over_lead(da: xr.DataArray) -> xr.DataArray:\n",
    "    # average away time/init_time & any lat/lon left, then squeeze to 1D lead\n",
    "    for d in (\"time\", \"init_time\"):\n",
    "        if d in da.dims:\n",
    "            da = da.mean(dim=d, keep_attrs=True, skipna=True)\n",
    "    for d in (\"latitude\", \"longitude\"):\n",
    "        if d in da.dims:\n",
    "            da = da.mean(dim=d, keep_attrs=True, skipna=True)\n",
    "    lead = get_lead_coord(da)\n",
    "    return da.transpose(lead).squeeze()\n",
    "\n",
    "def lead_days_from(da_1d: xr.DataArray) -> np.ndarray:\n",
    "    lead = get_lead_coord(da_1d)\n",
    "    lt = np.asarray(da_1d[lead].values)\n",
    "    # support timedelta64 or ints (hours)\n",
    "    if np.issubdtype(lt.dtype, np.timedelta64):\n",
    "        x = (lt / np.timedelta64(1, \"D\")).astype(float)\n",
    "    else:\n",
    "        # assume hours\n",
    "        x = (lt.astype(float) / 24.0)\n",
    "    return x.reshape(-1)\n",
    "\n",
    "if forecast_ds is None or era5_ds is None:\n",
    "    display(Markdown(\"‚ÑπÔ∏è Load forecast (Cell 4) and ERA5 truth (Cell 5) first.\"))\n",
    "else:\n",
    "    region_selector = widgets.Dropdown(options=list(DOMAIN_DEFINITIONS.keys()),\n",
    "                                       value=\"Global\", description=\"Region:\")\n",
    "    metric_selector  = widgets.Dropdown(options=[\"RMSE\",\"ACC\"],\n",
    "                                        value=\"ACC\", description=\"Metric:\")\n",
    "    available = [v for v in vars_for_this_run if v in forecast_ds.data_vars and v in era5_ds.data_vars]\n",
    "    if not available:\n",
    "        available = [v for v in [\"2t\",\"z_500\",\"tp\"] if v in forecast_ds.data_vars and v in era5_ds.data_vars]\n",
    "    variable_selector = widgets.Dropdown(options=available, value=available[0], description=\"Variable:\")\n",
    "    run_btn = widgets.Button(description=\"Run Verification\", button_style=\"success\")\n",
    "    out = widgets.Output()\n",
    "    display(region_selector, metric_selector, variable_selector, run_btn, out)\n",
    "\n",
    "    def on_run(_):\n",
    "        out.clear_output(wait=True)\n",
    "        with out:\n",
    "            try:\n",
    "                region_name = region_selector.value\n",
    "                box         = DOMAIN_DEFINITIONS[region_name]\n",
    "                metric      = metric_selector.value\n",
    "                var         = variable_selector.value\n",
    "\n",
    "                # 1) WB-X metrics on full grid (no pre-slicing)\n",
    "                stats = run_verification(forecast_ds, era5_ds, metric, [var])\n",
    "\n",
    "                # 2) Region aggregation AFTER metric computation\n",
    "                if metric == \"RMSE\":\n",
    "                    se = stats[\"SquaredError\"][var]  # (time|init_time, lead, lat, lon)\n",
    "                    se_reg = se.sel(\n",
    "                        latitude=_lat_slice_for(se, box),\n",
    "                        longitude=box[\"longitude\"]\n",
    "                    ).mean(dim=[\"latitude\",\"longitude\"], skipna=True)\n",
    "                    rmse = np.sqrt(se_reg)                 # (time|init_time, lead)\n",
    "                    line = to_1d_over_lead(rmse)           # -> 1D over lead\n",
    "                    ylab, title = \"RMSE\", f\"RMSE ‚Äî {var} ‚Äî {region_name}\"\n",
    "                else:\n",
    "                    pa = stats[\"SquaredPredictionAnomaly\"][var]\n",
    "                    ta = stats[\"SquaredTargetAnomaly\"][var]\n",
    "                    co = stats[\"AnomalyCovariance\"][var]\n",
    "                    lat_sl = _lat_slice_for(pa, box)\n",
    "                    pa = pa.sel(latitude=lat_sl, longitude=box[\"longitude\"]).mean(dim=[\"latitude\",\"longitude\"], skipna=True)\n",
    "                    ta = ta.sel(latitude=lat_sl, longitude=box[\"longitude\"]).mean(dim=[\"latitude\",\"longitude\"], skipna=True)\n",
    "                    co = co.sel(latitude=lat_sl, longitude=box[\"longitude\"]).mean(dim=[\"latitude\",\"longitude\"], skipna=True)\n",
    "                    acc  = co / np.sqrt(pa * ta)           # (time|init_time, lead)\n",
    "                    line = to_1d_over_lead(acc)            # -> 1D over lead\n",
    "                    ylab, title = \"ACC\",  f\"ACC ‚Äî {var} ‚Äî {region_name}\"\n",
    "\n",
    "                # 3) Build x, y and mask non-finite values\n",
    "                x = lead_days_from(line)                              # (n_leads,)\n",
    "                y = np.asarray(line.values, dtype=float).reshape(-1)  # (n_leads,)\n",
    "                m = np.isfinite(x) & np.isfinite(y)\n",
    "                x_plot, y_plot = x[m], y[m]\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(8,4))\n",
    "                if x_plot.size > 0:\n",
    "                    ax.plot(x_plot, y_plot)\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, \"No finite values to plot\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "                ax.set_title(title)\n",
    "                ax.set_xlabel(\"Forecast time (days)\")\n",
    "                ax.set_ylabel(ylab)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                plt.show()\n",
    "\n",
    "                # Optional quick debug readout\n",
    "                print(f\"lead len: {x.size}, finite points: {x_plot.size}, y range: {np.nanmin(y):.3g}..{np.nanmax(y):.3g}\")\n",
    "\n",
    "                display(Markdown(\"‚úÖ Done\"))\n",
    "            except Exception as e:\n",
    "                display(Markdown(f\"‚ùå Error: `{e}`\"))\n",
    "\n",
    "    run_btn.on_click(on_run)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIFS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
